{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baker-jr-john/automated-summary-evaluation-llm/blob/main/04_Summary_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpii-tVVZhZl"
      },
      "source": [
        "# üìù Summary Evaluation System - Live Demo\n",
        "**John Baker | EDUC 6192 Final Project | December 2025**\n",
        "\n",
        "This notebook creates a live Gradio interface for evaluating student summaries of \"The Challenge of Exploring Venus\" using Llama 3.1 8B.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKevf8a2ZhZr"
      },
      "source": [
        "## Step 1: Install Dependencies\n",
        "Run this cell first (takes ~2 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWdQNR2aZhZs",
        "outputId": "2e054d1a-b375-47ec-d32d-f2896311141b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dependencies installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers accelerate bitsandbytes gradio torch\n",
        "\n",
        "print(\"‚úÖ Dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_WirNIRZhZu"
      },
      "source": [
        "## Step 2: Load the Model\n",
        "This takes 2-3 minutes. You'll see progress updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWwVU2uaZhZv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import json\n",
        "import re\n",
        "import gc\n",
        "import gradio as gr\n",
        "\n",
        "print(\"Loading Llama 3.1 8B with 4-bit quantization...\")\n",
        "print(\"This takes 2-3 minutes. Please wait...\\n\")\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# 4-bit quantization for Colab free tier\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"[1/2] Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model\n",
        "print(\"[2/2] Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.float16\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Model loaded successfully!\")\n",
        "print(f\"   Device: {model.device}\")\n",
        "print(f\"   Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVk_liANZhZx"
      },
      "source": [
        "## Step 3: Define Source Text and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdJ9fuv0ZhZx",
        "outputId": "e794a824-8219-4fd1-ed5f-f9b0ab0aab56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Source text loaded\n"
          ]
        }
      ],
      "source": [
        "# Source text from \"The Challenge of Exploring Venus\"\n",
        "VENUS_SOURCE_TEXT = \"\"\"THE CHALLENGE OF EXPLORING VENUS\n",
        "\n",
        "Venus, sometimes called the \"Evening Star,\" is one of the brightest points of light in the night sky, making it simple for even an amateur stargazer to spot. However, this nickname is misleading since Venus is actually a planet. While Venus is simple to see from the distant but safe vantage point of Earth, it has proved a very challenging place to examine more closely.\n",
        "\n",
        "Often referred to as Earth's \"twin,\" Venus is the closest planet to Earth in terms of density and size, and occasionally the closest in distance too. Earth and Venus, along with Mars, our other planetary neighbor, orbit the sun at different speeds. These differences in speed mean that sometimes we are closer to Mars and other times to Venus. Because Venus is sometimes right around the corner‚Äîin space terms‚Äîhumans have sent numerous spacecraft to land on this cloud-draped world. Each previous mission was unmanned, and for good reason, since no spacecraft survived the landing for more than a few hours. Maybe this issue explains why not a single spaceship has touched down on Venus in more than three decades. Numerous factors contribute to Venus's reputation as a challenging planet for humans to study, despite its proximity to us.\n",
        "\n",
        "A thick atmosphere of almost 97 percent carbon dioxide blankets Venus. Even more challenging are the clouds of highly corrosive sulfuric acid in Venus's atmosphere. On the planet's surface, temperatures average over 800 degrees Fahrenheit, and the atmospheric pressure is 90 times greater than what we experience on our own planet. These conditions are far more extreme than anything humans encounter on Earth; such an environment would crush even a submarine accustomed to diving to the deepest parts of our oceans and would liquefy many metals. Also notable, Venus has the hottest surface temperature of any planet in our solar system, even though Mercury is closer to our sun. Beyond high pressure and heat, Venusian geology and weather present additional impediments like erupting volcanoes, powerful earthquakes, and frequent lightning strikes to probes seeking to land on its surface.\n",
        "\n",
        "If our sister is so inhospitable, why are scientists even discussing further visits to its surface? Astronomers are fascinated by Venus because it may well once have been the most Earth-like planet in our solar system. Long ago, Venus was probably covered largely with oceans and could have supported various forms of life, just like Earth. Today, Venus still has some features that are analogous to those on Earth. The planet has a surface of rocky sediment and includes familiar features such as valleys, mountains, and craters. Furthermore, recall that Venus can sometimes be our nearest option for a planetary visit, a crucial consideration given the long time frames of space travel. The value of returning to Venus seems indisputable, but what are the options for making such a mission both safe and scientifically productive?\n",
        "\n",
        "The National Aeronautics and Space Administration (NASA) has one particularly compelling idea for sending humans to study Venus. NASA's possible solution to the hostile conditions on the surface of Venus would allow scientists to float above the fray. Imagine a blimp-like vehicle hovering 30 or so miles above the roiling Venusian landscape. Just as our jet airplanes travel at a higher altitude to fly over many storms, a vehicle hovering over Venus would avoid the unfriendly ground conditions by staying up and out of the way. At thirty-plus miles above the surface, temperatures would still be toasty at around 170 degrees Fahrenheit, but the air pressure would be close to that of sea level on Earth. Solar power would be plentiful, and radiation would not exceed Earth's levels. Not easy conditions, but survivable for humans.\n",
        "\n",
        "However, peering at Venus from a ship orbiting or hovering safely far above the planet can provide only limited insight into ground conditions, rendering standard forms of photography and videography ineffective. More importantly, researchers cannot take samples of rock, gas, or anything else from a distance. Therefore, scientists seeking to conduct a thorough mission to understand Venus would need to get up close and personal despite the risks. Or maybe we should think of them as challenges. Many researchers are working on innovations that would allow our machines to last long enough to contribute meaningfully to our knowledge of Venus.\n",
        "\n",
        "NASA is working on other approaches to studying Venus. For example, some simplified electronics made of silicon carbide have been tested in a chamber simulating the chaos of Venus's surface and have lasted for three weeks in such conditions. Another project is looking back at an old technology called mechanical computers. These devices were first envisioned in the 1800s and played an important role in the 1940s during World War II. The thought of computers existing in those days may sound shocking, but these devices made calculations by using gears and levers and did not require electronics at all. Modern computers are enormously powerful, flexible, and quick, but tend to be more delicate when it comes to extreme physical conditions. Just imagine exposing a cell phone or tablet to acid or heat capable of melting tin. By comparison, systems that use mechanical parts can be made more resistant to pressure, heat, and other forces.\n",
        "\n",
        "Striving to meet the challenge presented by Venus has value, not only because of the insight to be gained on the planet itself, but also because human curiosity will likely lead us into many equally intimidating endeavors. Our travels on Earth and beyond should not be limited by dangers and doubts but should be expanded to meet the very edges of imagination and innovation.\"\"\"\n",
        "\n",
        "print(\"‚úÖ Source text loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IVUd3-RZhZy",
        "outputId": "65ea976c-2c1b-4680-d36d-9fba7a6c283d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Final Cleanup Logic Applied\n"
          ]
        }
      ],
      "source": [
        "def create_evaluation_prompt(summary, source_text):\n",
        "    \"\"\"\n",
        "    Final Prompt: 'Innocent until proven guilty' logic for Conciseness.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"\"\"You are a strict teacher grading a student summary.\n",
        "Compare the summary CAREFULLY to the source text.\n",
        "\n",
        "SOURCE TEXT:\n",
        "{source_text}\n",
        "\n",
        "RUBRIC:\n",
        "ACCURACY (STRICT):\n",
        "5=Perfectly accurate.\n",
        "4=Minor details off.\n",
        "3=Mix of true/false.\n",
        "2=Contains \"hallucinations\" (facts not in text).\n",
        "1=Completely wrong.\n",
        "\n",
        "CONCISENESS (LENIENT):\n",
        "5=Effective Standard English. (Includes intro, transitions, and conclusion).\n",
        "4=Mostly efficient.\n",
        "3=Visibly redundant (repeats the exact same sentence twice).\n",
        "2=Excessively wordy.\n",
        "1=Rambling.\n",
        "\n",
        "---\n",
        "INSTRUCTIONS:\n",
        "1. ACCURACY: Be strict. If the summary claims things not in the source (like \"fast technology\", \"humans living there\", or \"oceans\"), the score MUST be 2 or 1.\n",
        "2. CONCISENESS: Be generous. Start at Score 5.\n",
        "   - Do NOT penalize for standard essay features (intro, conclusion, transitions).\n",
        "   - Do NOT penalize the student for discussing the author's repetition.\n",
        "   - Only give a low score if the student repeats the EXACT SAME thought multiple times in a row.\n",
        "\n",
        "FORMAT:\n",
        "Provide your response as a simple list. Do not use JSON. Use exactly this format:\n",
        "\n",
        "Completeness Score: <number>\n",
        "Completeness Feedback: <text>\n",
        "Accuracy Score: <number>\n",
        "Accuracy Feedback: <text>\n",
        "Coherence Score: <number>\n",
        "Coherence Feedback: <text>\n",
        "Conciseness Score: <number>\n",
        "Conciseness Feedback: <text>\n",
        "\n",
        "---\n",
        "STUDENT SUMMARY:\n",
        "{summary}\n",
        "\n",
        "YOUR EVALUATION:\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def evaluate_summary(summary, model, tokenizer, source_text):\n",
        "    \"\"\"\n",
        "    Evaluation Function - TOKEN SLICING + CLEANUP\n",
        "    \"\"\"\n",
        "\n",
        "    if not summary.strip():\n",
        "        return {\"error\": \"Please enter a summary to evaluate\"}\n",
        "\n",
        "    # 1. Clear Memory\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 2. Create Prompt\n",
        "    prompt = create_evaluation_prompt(summary, source_text)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    # 3. Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=600,\n",
        "            temperature=0.1,\n",
        "            do_sample=True,\n",
        "            top_p=0.95,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Slice only new tokens\n",
        "    input_length = inputs[\"input_ids\"].shape[1]\n",
        "    generated_tokens = outputs[0][input_length:]\n",
        "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    # Sanitize\n",
        "    try:\n",
        "        response = response.encode('utf-8', 'ignore').decode('utf-8')\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # 4. Parse\n",
        "    return parse_line_items(response)\n",
        "\n",
        "\n",
        "def parse_line_items(text):\n",
        "    \"\"\"\n",
        "    Universal Parser with 'Chatty Cleanup'\n",
        "    Stops reading feedback if the model starts chatting (e.g., 'Note:', 'Let me know')\n",
        "    \"\"\"\n",
        "    if \"YOUR EVALUATION:\" in text:\n",
        "        text = text.split(\"YOUR EVALUATION:\")[-1]\n",
        "\n",
        "    keys = [\n",
        "        \"Completeness Score\", \"Completeness Feedback\",\n",
        "        \"Accuracy Score\", \"Accuracy Feedback\",\n",
        "        \"Coherence Score\", \"Coherence Feedback\",\n",
        "        \"Conciseness Score\", \"Conciseness Feedback\"\n",
        "    ]\n",
        "\n",
        "    # Find keys\n",
        "    found_keys = []\n",
        "    for key in keys:\n",
        "        pattern = rf\"{key}[:\\s]\"\n",
        "        match = re.search(pattern, text, re.IGNORECASE)\n",
        "        if match:\n",
        "            found_keys.append({\"key\": key, \"start\": match.start(), \"end\": match.end()})\n",
        "\n",
        "    found_keys.sort(key=lambda x: x[\"start\"])\n",
        "\n",
        "    results = {}\n",
        "    for i, item in enumerate(found_keys):\n",
        "        current_key = item[\"key\"]\n",
        "        start_content = item[\"end\"]\n",
        "\n",
        "        if i < len(found_keys) - 1:\n",
        "            end_content = found_keys[i + 1][\"start\"]\n",
        "        else:\n",
        "            end_content = len(text)\n",
        "\n",
        "        content = text[start_content:end_content].strip()\n",
        "        content = re.sub(r\"^[:\\-\\s]+\", \"\", content).strip()\n",
        "\n",
        "        # --- üßπ CLEANUP LOGIC ---\n",
        "        # If this is a feedback field, cut off common conversational markers\n",
        "        if \"Feedback\" in current_key:\n",
        "            # List of phrases that indicate the model has stopped grading and started chatting\n",
        "            stop_phrases = [\n",
        "                \"Note:\", \"Let me know\", \"I hope\", \"I'd be happy\", \"Best regards\",\n",
        "                \"My response:\", \"Completeness Score:\", \"---\"\n",
        "            ]\n",
        "            for phrase in stop_phrases:\n",
        "                if phrase in content:\n",
        "                    content = content.split(phrase)[0].strip()\n",
        "\n",
        "            # Also cut on double newlines if they look like paragraph breaks to a signature\n",
        "            # (Optional: keep if you want multi-paragraph feedback, but usually safer to cut)\n",
        "            if \"\\n\\n\" in content:\n",
        "                 content = content.split(\"\\n\\n\")[0].strip()\n",
        "        # ------------------------\n",
        "\n",
        "        results[current_key] = content\n",
        "\n",
        "    scores = {}\n",
        "    dims = [\"Completeness\", \"Accuracy\", \"Coherence\", \"Conciseness\"]\n",
        "\n",
        "    for dim in dims:\n",
        "        score_key = f\"{dim} Score\"\n",
        "        fb_key = f\"{dim} Feedback\"\n",
        "\n",
        "        raw_score = results.get(score_key, \"0\")\n",
        "        score_match = re.search(r\"\\d\", raw_score)\n",
        "        score = int(score_match.group(0)) if score_match else 0\n",
        "\n",
        "        feedback = results.get(fb_key, \"Could not parse feedback.\")\n",
        "        scores[dim.lower()] = {\"score\": score, \"feedback\": feedback}\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "def format_results(evaluation):\n",
        "    \"\"\"Format results for display\"\"\"\n",
        "    if \"error\" in evaluation:\n",
        "        return f\"‚ùå **Error:** {evaluation['error']}\\n\\n**Raw Output:**\\n{evaluation.get('raw', '')}\"\n",
        "\n",
        "    results = \"## üìä Evaluation Results\\n\\n\"\n",
        "\n",
        "    display_map = [\n",
        "        (\"Completeness\", \"completeness\", \"üìù\"),\n",
        "        (\"Accuracy\", \"accuracy\", \"‚úÖ\"),\n",
        "        (\"Coherence\", \"coherence\", \"üîó\"),\n",
        "        (\"Conciseness\", \"conciseness\", \"‚úÇÔ∏è\")\n",
        "    ]\n",
        "\n",
        "    for name, key, emoji in display_map:\n",
        "        if key in evaluation:\n",
        "            item = evaluation[key]\n",
        "            score = item.get(\"score\", 0)\n",
        "            feedback = item.get(\"feedback\", \"No feedback\")\n",
        "\n",
        "            bar = \"üü¶\" * score + \"‚¨ú\" * (5 - score)\n",
        "\n",
        "            results += f\"### {emoji} {name}: {score}/5\\n\"\n",
        "            results += f\"{bar}\\n\\n\"\n",
        "            results += f\"**Feedback:** {feedback}\\n\\n\"\n",
        "            results += \"---\\n\\n\"\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Final Cleanup Logic Applied\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U570FbXFZhZ3"
      },
      "source": [
        "## Step 4: Launch Gradio Interface\n",
        "This creates a shareable web interface for live evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "wmO0UkzqZhZ3",
        "outputId": "25794305-ff52-48e8-900f-6d9417c1009e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3116455829.py:7: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(title=\"Summary Evaluation System\", theme=gr.themes.Soft()) as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://44373e80ab84ae5c97.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://44373e80ab84ae5c97.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://44373e80ab84ae5c97.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "def gradio_evaluate(summary):\n",
        "    \"\"\"Wrapper for Gradio\"\"\"\n",
        "    evaluation = evaluate_summary(summary, model, tokenizer, VENUS_SOURCE_TEXT)\n",
        "    return format_results(evaluation)\n",
        "\n",
        "# Create interface\n",
        "with gr.Blocks(title=\"Summary Evaluation System\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üìù Summary Evaluation System\n",
        "    ### Automated Rubric-Based Feedback for \"The Challenge of Exploring Venus\"\n",
        "\n",
        "    **Instructions:** Paste a student summary below and click \"Evaluate\" to receive instant feedback.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            summary_input = gr.Textbox(\n",
        "                label=\"Student Summary\",\n",
        "                placeholder=\"Paste the summary here...\",\n",
        "                lines=10\n",
        "            )\n",
        "\n",
        "            evaluate_btn = gr.Button(\"üéØ Evaluate Summary\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column():\n",
        "            results_output = gr.Markdown(\n",
        "                value=\"*Results will appear here*\"\n",
        "            )\n",
        "\n",
        "    evaluate_btn.click(\n",
        "        fn=gradio_evaluate,\n",
        "        inputs=summary_input,\n",
        "        outputs=results_output\n",
        "    )\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    **Project:** Automated Summary Evaluation | **Developer:** John Baker | **Course:** EDUC 6192\n",
        "    \"\"\")\n",
        "\n",
        "# Launch with share=True for public URL\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7t85lg8ZhZ4"
      },
      "source": [
        "---\n",
        "## üéØ Next Steps for Demo Day (Dec 10)\n",
        "\n",
        "1. **Test with validation summaries** - Copy-paste from your scored set\n",
        "2. **Check consistency** - Run same summary multiple times\n",
        "3. **Document edge cases** - Note any parsing failures\n",
        "4. **Polish UI** - Adjust colors, add more examples\n",
        "5. **Prepare talking points** - Explain how rubric guides evaluation\n",
        "\n",
        "**Backup plan:** If Gradio has issues, you can always run `evaluate_summary()` directly and show results in notebook output."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}