{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "16rzCEir-3WyU2xJobpiuSnjGvjfBtkDB",
      "authorship_tag": "ABX9TyMHhT0MxMn7E6F86iOovREV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baker-jr-john/automated-summary-evaluation-llm/blob/main/02_Calibration_Tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Calibration Subset Selection\n",
        "# =============================================================================\n",
        "\n",
        "# =============================================================================\n",
        "# SETUP: Mount Google Drive\n",
        "# =============================================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION: Set your file paths\n",
        "# =============================================================================\n",
        "\n",
        "# UPDATE THESE PATHS to match your Google Drive structure:\n",
        "VALIDATION_DATASET_PATH = '/content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_1/data/dataset/validation_set_combined_60.csv'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_2/Phase2_Calibration/'\n",
        "\n",
        "# The script will create these files in OUTPUT_DIR:\n",
        "# - calibration_subset.csv\n",
        "# - calibration_practice_summaries.txt\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CALIBRATION SUBSET SELECTION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nReading from: {VALIDATION_DATASET_PATH}\")\n",
        "print(f\"Saving to: {OUTPUT_DIR}\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: Load and analyze dataset\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"\\n[STEP 1] Loading Dataset...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "df = pd.read_csv(VALIDATION_DATASET_PATH)\n",
        "\n",
        "print(f\"âœ“ Loaded {len(df)} summaries\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "\n",
        "print(\"\\n\\nDataset Distribution:\")\n",
        "print(f\"  Authentic (ASAP 2.0): {(df['synthetic_flag'] == False).sum()}\")\n",
        "print(f\"  Synthetic (GPT-4o-Mini): {(df['synthetic_flag'] == True).sum()}\")\n",
        "\n",
        "print(\"\\nScore distribution:\")\n",
        "score_dist = df['score'].value_counts().sort_index()\n",
        "for score, count in score_dist.items():\n",
        "    pct = (count / len(df)) * 100\n",
        "    print(f\"  Score {score}: {count:2d} summaries ({pct:4.1f}%)\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: Define selection strategy\n",
        "# =============================================================================\n",
        "print(\"\\n\\n[STEP 2] Selection Strategy\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "selection_plan = {\n",
        "    1: {'target': 2, 'authentic': 1, 'synthetic': 1},\n",
        "    2: {'target': 3, 'authentic': 2, 'synthetic': 1},\n",
        "    3: {'target': 3, 'authentic': 2, 'synthetic': 1},\n",
        "    4: {'target': 2, 'authentic': 1, 'synthetic': 1},\n",
        "    5: {'target': 1, 'authentic': 1, 'synthetic': 0},\n",
        "    6: {'target': 1, 'authentic': 1, 'synthetic': 0}\n",
        "}\n",
        "\n",
        "print(\"\\nWill select:\")\n",
        "for score, plan in selection_plan.items():\n",
        "    print(f\"  Score {score}: {plan['target']} summaries ({plan['authentic']} auth, {plan['synthetic']} synth)\")\n",
        "\n",
        "total_target = sum(plan['target'] for plan in selection_plan.values())\n",
        "print(f\"\\nTotal: {total_target} summaries for calibration\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: Execute selection\n",
        "# =============================================================================\n",
        "print(\"\\n\\n[STEP 3] Selecting Summaries\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "calibration_subset = []\n",
        "\n",
        "# Score 1: 1 authentic + 1 synthetic\n",
        "score_1_df = df[df['score'] == 1]\n",
        "cal_1_auth = score_1_df[score_1_df['synthetic_flag'] == False].iloc[0]\n",
        "cal_1_synth = score_1_df[score_1_df['synthetic_flag'] == True].iloc[0]\n",
        "calibration_subset.extend([cal_1_auth, cal_1_synth])\n",
        "print(f\"Score 1: Selected {cal_1_auth['essay_id']} (auth), {cal_1_synth['essay_id']} (synth)\")\n",
        "\n",
        "# Score 2: 2 authentic + 1 synthetic\n",
        "score_2_df = df[df['score'] == 2]\n",
        "cal_2_auth = score_2_df[score_2_df['synthetic_flag'] == False].iloc[0:2]\n",
        "cal_2_synth = score_2_df[score_2_df['synthetic_flag'] == True].iloc[0]\n",
        "calibration_subset.extend([cal_2_auth.iloc[0], cal_2_auth.iloc[1], cal_2_synth])\n",
        "print(f\"Score 2: Selected {cal_2_auth.iloc[0]['essay_id']}, {cal_2_auth.iloc[1]['essay_id']} (auth), {cal_2_synth['essay_id']} (synth)\")\n",
        "\n",
        "# Score 3: 2 authentic + 1 synthetic\n",
        "score_3_df = df[df['score'] == 3]\n",
        "cal_3_auth = score_3_df[score_3_df['synthetic_flag'] == False].iloc[0:2]\n",
        "cal_3_synth = score_3_df[score_3_df['synthetic_flag'] == True].iloc[0]\n",
        "calibration_subset.extend([cal_3_auth.iloc[0], cal_3_auth.iloc[1], cal_3_synth])\n",
        "print(f\"Score 3: Selected {cal_3_auth.iloc[0]['essay_id']}, {cal_3_auth.iloc[1]['essay_id']} (auth), {cal_3_synth['essay_id']} (synth)\")\n",
        "\n",
        "# Score 4: 1 authentic + 1 synthetic\n",
        "score_4_df = df[df['score'] == 4]\n",
        "cal_4_auth = score_4_df[score_4_df['synthetic_flag'] == False].iloc[0]\n",
        "cal_4_synth = score_4_df[score_4_df['synthetic_flag'] == True].iloc[0]\n",
        "calibration_subset.extend([cal_4_auth, cal_4_synth])\n",
        "print(f\"Score 4: Selected {cal_4_auth['essay_id']} (auth), {cal_4_synth['essay_id']} (synth)\")\n",
        "\n",
        "# Score 5: 1 authentic\n",
        "score_5_df = df[df['score'] == 5]\n",
        "cal_5_auth = score_5_df[score_5_df['synthetic_flag'] == False].iloc[0]\n",
        "calibration_subset.append(cal_5_auth)\n",
        "print(f\"Score 5: Selected {cal_5_auth['essay_id']} (auth)\")\n",
        "\n",
        "# Score 6: 1 authentic (only one available)\n",
        "score_6_df = df[df['score'] == 6]\n",
        "cal_6 = score_6_df.iloc[0]\n",
        "calibration_subset.append(cal_6)\n",
        "print(f\"Score 6: Selected {cal_6['essay_id']} (auth)\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: Create calibration DataFrame\n",
        "# =============================================================================\n",
        "print(\"\\n\\n[STEP 4] Creating Calibration DataFrame\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "cal_df = pd.DataFrame(calibration_subset)\n",
        "cal_df = cal_df.reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nâœ“ Created DataFrame with {len(cal_df)} summaries\")\n",
        "print(f\"\\nScore distribution in calibration set:\")\n",
        "print(cal_df['score'].value_counts().sort_index())\n",
        "print(f\"\\nAuthentic: {(cal_df['synthetic_flag'] == False).sum()}\")\n",
        "print(f\"Synthetic: {(cal_df['synthetic_flag'] == True).sum()}\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: Display details\n",
        "# =============================================================================\n",
        "print(\"\\n\\n[STEP 5] Calibration Subset Details\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"\\n{'#':<3} {'Essay ID':<25} {'Score':<6} {'Source':<8} {'Words':<6} {'Error Pattern':<45}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for idx, row in cal_df.iterrows():\n",
        "    source = \"Synthetic\" if row['synthetic_flag'] else \"Authentic\"\n",
        "    error = row['target_error_pattern'] if pd.notna(row['target_error_pattern']) else \"N/A\"\n",
        "    print(f\"{idx+1:<3} {row['essay_id']:<25} {row['score']:<6} {source:<8} {row['word_count']:<6} {error[:45]:<45}\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 6: Save CSV to Google Drive\n",
        "# =============================================================================\n",
        "print(\"\\n\\n[STEP 6] Saving Calibration Subset CSV\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "cal_csv_path = os.path.join(OUTPUT_DIR, 'calibration_subset.csv')\n",
        "cal_df.to_csv(cal_csv_path, index=False)\n",
        "print(f\"âœ“ Saved: {cal_csv_path}\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 7: Create practice summaries document\n",
        "# =============================================================================\n",
        "print(\"\\n\\n[STEP 7] Creating Practice Summaries Document\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "output_lines = []\n",
        "output_lines.append(\"=\" * 80)\n",
        "output_lines.append(\"CALIBRATION PRACTICE SET - 12 SUMMARIES\")\n",
        "output_lines.append(\"=\" * 80)\n",
        "output_lines.append(\"\\nInstructions:\")\n",
        "output_lines.append(\"1. Score each summary across all 4 dimensions WITHOUT looking at benchmark scores\")\n",
        "output_lines.append(\"2. Use your rubric and document your reasoning\")\n",
        "output_lines.append(\"3. After scoring all 12, compare with the benchmark scores\")\n",
        "output_lines.append(\"4. Analyze discrepancies to refine your rubric interpretation\")\n",
        "output_lines.append(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "for idx, row in cal_df.iterrows():\n",
        "    practice_num = idx + 1\n",
        "\n",
        "    output_lines.append(f\"\\n{'='*80}\")\n",
        "    output_lines.append(f\"PRACTICE_{practice_num:02d}: {row['essay_id']}\")\n",
        "    output_lines.append(f\"{'='*80}\")\n",
        "    output_lines.append(f\"Source: {'Synthetic' if row['synthetic_flag'] else 'Authentic'}\")\n",
        "    output_lines.append(f\"Word Count: {row['word_count']}\")\n",
        "    if pd.notna(row['target_error_pattern']):\n",
        "        output_lines.append(f\"Error Pattern: {row['target_error_pattern']}\")\n",
        "\n",
        "    output_lines.append(f\"\\n{'-'*80}\")\n",
        "    output_lines.append(\"SUMMARY TEXT:\")\n",
        "    output_lines.append(f\"{'-'*80}\\n\")\n",
        "    output_lines.append(row['full_text'])\n",
        "    output_lines.append(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Save to text file\n",
        "practice_txt_path = os.path.join(OUTPUT_DIR, 'calibration_practice_summaries.txt')\n",
        "with open(practice_txt_path, 'w', encoding='utf-8') as f:\n",
        "    f.write('\\n'.join(output_lines))\n",
        "\n",
        "print(f\"âœ“ Saved: {practice_txt_path}\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 8: Create Practice IDs reference\n",
        "# =============================================================================\n",
        "print(\"\\n\\n[STEP 8] Practice IDs for Calibration Tracker\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nCopy these IDs into your Calibration_Tracker.xlsx:\")\n",
        "print()\n",
        "for i, essay_id in enumerate(cal_df['essay_id'], 1):\n",
        "    print(f\"PRACTICE_R1_{i:02d}: {essay_id}\")\n",
        "\n",
        "# Save to a separate reference file\n",
        "practice_ids_path = os.path.join(OUTPUT_DIR, 'calibration_practice_ids.txt')\n",
        "with open(practice_ids_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"Practice IDs for Calibration Tracker\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    f.write(\"Use these in your Calibration_Tracker.xlsx:\\n\\n\")\n",
        "    for i, essay_id in enumerate(cal_df['essay_id'], 1):\n",
        "        f.write(f\"PRACTICE_R1_{i:02d}: {essay_id}\\n\")\n",
        "\n",
        "print(f\"\\nâœ“ Saved: {practice_ids_path}\")\n",
        "\n",
        "# =============================================================================\n",
        "# COMPLETION\n",
        "# =============================================================================\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"CALIBRATION SUBSET SELECTION COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nâœ“ Files saved to: {OUTPUT_DIR}\")\n",
        "print(f\"  â€¢ calibration_subset.csv (metadata)\")\n",
        "print(f\"  â€¢ calibration_practice_summaries.txt (full texts)\")\n",
        "print(f\"  â€¢ calibration_practice_ids.txt (IDs for tracker)\")\n",
        "\n",
        "print(f\"\\nâœ“ Selected {len(cal_df)} summaries:\")\n",
        "print(f\"  â€¢ All 6 score levels covered\")\n",
        "print(f\"  â€¢ {(cal_df['synthetic_flag'] == False).sum()} authentic, {(cal_df['synthetic_flag'] == True).sum()} synthetic\")\n",
        "print(f\"  â€¢ Word count range: {cal_df['word_count'].min()}-{cal_df['word_count'].max()}\")\n",
        "\n",
        "print(\"\\nðŸ“‹ Next steps:\")\n",
        "print(\"  1. Download the three files from your Google Drive\")\n",
        "print(\"  2. Review the practice summaries document\")\n",
        "print(\"  3. Begin Phase 1 of calibration (Rubric Study)\")\n",
        "print(\"  4. Use the practice IDs to update your Calibration_Tracker.xlsx\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feQW4BYuS4E6",
        "outputId": "0f2a6394-7c60-4b89-82bb-26dbcc6eb3e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "================================================================================\n",
            "CALIBRATION SUBSET SELECTION\n",
            "================================================================================\n",
            "\n",
            "Reading from: /content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_1/data/dataset/validation_set_combined_60.csv\n",
            "Saving to: /content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_2/Phase2_Calibration/\n",
            "\n",
            "[STEP 1] Loading Dataset...\n",
            "--------------------------------------------------------------------------------\n",
            "âœ“ Loaded 60 summaries\n",
            "\n",
            "Columns: ['essay_id', 'score', 'full_text', 'assignment', 'prompt_name', 'economically_disadvantaged', 'student_disability_status', 'ell_status', 'race_ethnicity', 'gender', 'source_text_1', 'source_text_2', 'source_text_3', 'source_text_4', 'synthetic_flag', 'target_error_pattern', 'generation_date', 'generation_model', 'word_count']\n",
            "\n",
            "\n",
            "Dataset Distribution:\n",
            "  Authentic (ASAP 2.0): 37\n",
            "  Synthetic (GPT-4o-Mini): 23\n",
            "\n",
            "Score distribution:\n",
            "  Score 1:  8 summaries (13.3%)\n",
            "  Score 2: 18 summaries (30.0%)\n",
            "  Score 3: 20 summaries (33.3%)\n",
            "  Score 4: 10 summaries (16.7%)\n",
            "  Score 5:  3 summaries ( 5.0%)\n",
            "  Score 6:  1 summaries ( 1.7%)\n",
            "\n",
            "\n",
            "[STEP 2] Selection Strategy\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Will select:\n",
            "  Score 1: 2 summaries (1 auth, 1 synth)\n",
            "  Score 2: 3 summaries (2 auth, 1 synth)\n",
            "  Score 3: 3 summaries (2 auth, 1 synth)\n",
            "  Score 4: 2 summaries (1 auth, 1 synth)\n",
            "  Score 5: 1 summaries (1 auth, 0 synth)\n",
            "  Score 6: 1 summaries (1 auth, 0 synth)\n",
            "\n",
            "Total: 12 summaries for calibration\n",
            "\n",
            "\n",
            "[STEP 3] Selecting Summaries\n",
            "--------------------------------------------------------------------------------\n",
            "Score 1: Selected AAAVUP14319000017665 (auth), SYNTH_V_01_S1 (synth)\n",
            "Score 2: Selected AAAVUP14319000153727, AAAVUP14319000099170 (auth), SYNTH_V_09_S2 (synth)\n",
            "Score 3: Selected AAAVUP14319000141935, AAAVUP14319000151934 (auth), SYNTH_V_18_S3 (synth)\n",
            "Score 4: Selected AAAVUP14319000017185 (auth), SYNTH_V_21_S4 (synth)\n",
            "Score 5: Selected AAAVUP14319000033223 (auth)\n",
            "Score 6: Selected AAAVUP14319000070539 (auth)\n",
            "\n",
            "\n",
            "[STEP 4] Creating Calibration DataFrame\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "âœ“ Created DataFrame with 12 summaries\n",
            "\n",
            "Score distribution in calibration set:\n",
            "score\n",
            "1    2\n",
            "2    3\n",
            "3    3\n",
            "4    2\n",
            "5    1\n",
            "6    1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Authentic: 8\n",
            "Synthetic: 4\n",
            "\n",
            "\n",
            "[STEP 5] Calibration Subset Details\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "#   Essay ID                  Score  Source   Words  Error Pattern                                \n",
            "--------------------------------------------------------------------------------\n",
            "1   AAAVUP14319000017665      1      Authentic 387    Authentic student work                       \n",
            "2   SYNTH_V_01_S1             1      Synthetic 135    Severe incompleteness + fabrication          \n",
            "3   AAAVUP14319000153727      2      Authentic 286    Authentic student work                       \n",
            "4   AAAVUP14319000099170      2      Authentic 158    Authentic student work                       \n",
            "5   SYNTH_V_09_S2             2      Synthetic 204    Shallow coverage - lists facts without connec\n",
            "6   AAAVUP14319000141935      3      Authentic 263    Authentic student work                       \n",
            "7   AAAVUP14319000151934      3      Authentic 499    Authentic student work                       \n",
            "8   SYNTH_V_18_S3             3      Synthetic 251    Good summary with minor coherence gaps       \n",
            "9   AAAVUP14319000017185      4      Authentic 513    Authentic student work                       \n",
            "10  SYNTH_V_21_S4             4      Synthetic 257    Strong summary, minor accuracy detail        \n",
            "11  AAAVUP14319000033223      5      Authentic 732    Authentic student work                       \n",
            "12  AAAVUP14319000070539      6      Authentic 753    Authentic student work                       \n",
            "\n",
            "\n",
            "[STEP 6] Saving Calibration Subset CSV\n",
            "--------------------------------------------------------------------------------\n",
            "âœ“ Saved: /content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_2/Phase2_Calibration/calibration_subset.csv\n",
            "\n",
            "\n",
            "[STEP 7] Creating Practice Summaries Document\n",
            "--------------------------------------------------------------------------------\n",
            "âœ“ Saved: /content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_2/Phase2_Calibration/calibration_practice_summaries.txt\n",
            "\n",
            "\n",
            "[STEP 8] Practice IDs for Calibration Tracker\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Copy these IDs into your Calibration_Tracker.xlsx:\n",
            "\n",
            "PRACTICE_R1_01: AAAVUP14319000017665\n",
            "PRACTICE_R1_02: SYNTH_V_01_S1\n",
            "PRACTICE_R1_03: AAAVUP14319000153727\n",
            "PRACTICE_R1_04: AAAVUP14319000099170\n",
            "PRACTICE_R1_05: SYNTH_V_09_S2\n",
            "PRACTICE_R1_06: AAAVUP14319000141935\n",
            "PRACTICE_R1_07: AAAVUP14319000151934\n",
            "PRACTICE_R1_08: SYNTH_V_18_S3\n",
            "PRACTICE_R1_09: AAAVUP14319000017185\n",
            "PRACTICE_R1_10: SYNTH_V_21_S4\n",
            "PRACTICE_R1_11: AAAVUP14319000033223\n",
            "PRACTICE_R1_12: AAAVUP14319000070539\n",
            "\n",
            "âœ“ Saved: /content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_2/Phase2_Calibration/calibration_practice_ids.txt\n",
            "\n",
            "\n",
            "================================================================================\n",
            "CALIBRATION SUBSET SELECTION COMPLETE\n",
            "================================================================================\n",
            "\n",
            "âœ“ Files saved to: /content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_2/Phase2_Calibration/\n",
            "  â€¢ calibration_subset.csv (metadata)\n",
            "  â€¢ calibration_practice_summaries.txt (full texts)\n",
            "  â€¢ calibration_practice_ids.txt (IDs for tracker)\n",
            "\n",
            "âœ“ Selected 12 summaries:\n",
            "  â€¢ All 6 score levels covered\n",
            "  â€¢ 8 authentic, 4 synthetic\n",
            "  â€¢ Word count range: 135-753\n",
            "\n",
            "ðŸ“‹ Next steps:\n",
            "  1. Download the three files from your Google Drive\n",
            "  2. Review the practice summaries document\n",
            "  3. Begin Phase 1 of calibration (Rubric Study)\n",
            "  4. Use the practice IDs to update your Calibration_Tracker.xlsx\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Generate Calibration Benchmark Scores Answer Key\n",
        "Reads calibration_subset.csv and creates formatted answer key file\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Configuration\n",
        "CALIBRATION_SUBSET_PATH = \"/content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_2/Phase2_Calibration/calibration_subset.csv\"  # Adjust path as needed\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_2/Phase2_Calibration/CALIBRATION_BENCHMARK_SCORES.txt\"\n",
        "\n",
        "def create_benchmark_scores_file(input_csv, output_txt):\n",
        "    \"\"\"\n",
        "    Create formatted benchmark scores file from calibration subset CSV.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    input_csv : str\n",
        "        Path to calibration_subset.csv\n",
        "    output_txt : str\n",
        "        Path for output benchmark scores file\n",
        "    \"\"\"\n",
        "\n",
        "    # Read calibration subset\n",
        "    df = pd.read_csv(input_csv)\n",
        "\n",
        "    # Define practice IDs and their roles\n",
        "    exemplars = {\n",
        "        'PRACTICE_01': 'Not used for blind practice - reference only',\n",
        "        'PRACTICE_02': 'EXEMPLAR - analyzed in detail in EXEMPLAR_ANALYSIS_GUIDE.md',\n",
        "        'PRACTICE_07': 'EXEMPLAR - analyzed in detail in EXEMPLAR_ANALYSIS_GUIDE.md',\n",
        "        'PRACTICE_11': 'EXEMPLAR - analyzed in detail in EXEMPLAR_ANALYSIS_GUIDE.md'\n",
        "    }\n",
        "\n",
        "    round_1 = ['PRACTICE_03', 'PRACTICE_04', 'PRACTICE_05', 'PRACTICE_06']\n",
        "    round_2 = ['PRACTICE_08', 'PRACTICE_09', 'PRACTICE_10', 'PRACTICE_12']\n",
        "\n",
        "    # Create practice_id column if it doesn't exist\n",
        "    if 'practice_id' not in df.columns:\n",
        "        # Create practice IDs based on row order\n",
        "        df['practice_id'] = [f'PRACTICE_{str(i+1).zfill(2)}' for i in range(len(df))]\n",
        "\n",
        "    # Build the output content\n",
        "    content = []\n",
        "\n",
        "    # Header\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"CALIBRATION PRACTICE SUMMARIES - BENCHMARK SCORES (ANSWER KEY)\")\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"\")\n",
        "    content.append(\"DO NOT LOOK AT THIS FILE UNTIL YOU HAVE SCORED ALL 9 PRACTICE SUMMARIES BLIND!\")\n",
        "    content.append(\"\")\n",
        "    content.append(\"Instructions for Use:\")\n",
        "    content.append(\"1. Score all 9 practice summaries (PRACTICE_03 through PRACTICE_06, and\")\n",
        "    content.append(\"   PRACTICE_08 through PRACTICE_12) WITHOUT looking at this file\")\n",
        "    content.append(\"2. Record your scores in Calibration_Tracker.xlsx\")\n",
        "    content.append(\"3. AFTER completing all 9, open this file to compare your scores\")\n",
        "    content.append(\"4. Calculate agreement metrics and analyze discrepancies\")\n",
        "    content.append(\"\")\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"\")\n",
        "\n",
        "    # Exemplar summaries section\n",
        "    content.append(\"EXEMPLAR SUMMARIES (Study These First - Scores Already Known)\")\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"\")\n",
        "\n",
        "    for practice_id, note in exemplars.items():\n",
        "        row = df[df['practice_id'] == practice_id].iloc[0]\n",
        "        content.append(f\"{practice_id}: {row['essay_id']}\")\n",
        "        content.append(f\"Benchmark Score: {row['score']}\")\n",
        "        content.append(f\"Source: {'Authentic' if row['synthetic_flag'] == 0 else 'Synthetic'}\")\n",
        "        content.append(f\"Note: {note}\")\n",
        "        content.append(\"\")\n",
        "\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"\")\n",
        "\n",
        "    # Practice Round 1\n",
        "    content.append(\"PRACTICE ROUND 1 - BLIND SCORING (Complete First)\")\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"\")\n",
        "\n",
        "    for practice_id in round_1:\n",
        "        row = df[df['practice_id'] == practice_id].iloc[0]\n",
        "        content.append(f\"{practice_id}: {row['essay_id']}\")\n",
        "        content.append(f\"Benchmark Score: {row['score']}\")\n",
        "        content.append(f\"Source: {'Authentic' if row['synthetic_flag'] == 0 else 'Synthetic'}\")\n",
        "        content.append(f\"Word Count: {row['word_count']}\")\n",
        "        if row['synthetic_flag'] == 1 and pd.notna(row['target_error_pattern']):\n",
        "            content.append(f\"Error Pattern: {row['target_error_pattern']}\")\n",
        "        content.append(\"\")\n",
        "\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"\")\n",
        "\n",
        "    # Practice Round 2\n",
        "    content.append(\"PRACTICE ROUND 2 - BLIND SCORING (Complete Second)\")\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"\")\n",
        "\n",
        "    for practice_id in round_2:\n",
        "        row = df[df['practice_id'] == practice_id].iloc[0]\n",
        "        content.append(f\"{practice_id}: {row['essay_id']}\")\n",
        "        content.append(f\"Benchmark Score: {row['score']}\")\n",
        "        content.append(f\"Source: {'Authentic' if row['synthetic_flag'] == 0 else 'Synthetic'}\")\n",
        "        content.append(f\"Word Count: {row['word_count']}\")\n",
        "        if row['synthetic_flag'] == 1 and pd.notna(row['target_error_pattern']):\n",
        "            content.append(f\"Error Pattern: {row['target_error_pattern']}\")\n",
        "        content.append(\"\")\n",
        "\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"\")\n",
        "\n",
        "    # Score distribution\n",
        "    content.append(\"SCORE DISTRIBUTION IN PRACTICE SET\")\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"\")\n",
        "\n",
        "    score_counts = df['score'].value_counts().sort_index()\n",
        "    for score, count in score_counts.items():\n",
        "        practice_ids = df[df['score'] == score]['practice_id'].tolist()\n",
        "        ids_str = ', '.join(practice_ids)\n",
        "\n",
        "        # Identify exemplars\n",
        "        exemplar_ids = [pid for pid in practice_ids if pid in exemplars]\n",
        "        if exemplar_ids:\n",
        "            ids_str += f\" ({', '.join([f'{pid} - exemplar' for pid in exemplar_ids])})\"\n",
        "\n",
        "        content.append(f\"Score {score}: {count} {'summary' if count == 1 else 'summaries'} ({ids_str})\")\n",
        "\n",
        "    content.append(\"\")\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"\")\n",
        "\n",
        "    # Agreement metrics section\n",
        "    content.append(\"AGREEMENT METRICS TO CALCULATE\")\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"\")\n",
        "    content.append(\"After comparing your scores to these benchmarks:\")\n",
        "    content.append(\"\")\n",
        "    content.append(\"1. EXACT AGREEMENT: How many summaries did you score exactly the same?\")\n",
        "    content.append(\"   Target: â‰¥ 60% (at least 6 out of 9)\")\n",
        "    content.append(\"\")\n",
        "    content.append(\"2. ADJACENT AGREEMENT: How many were within Â±1 point?\")\n",
        "    content.append(\"   Target: > 85% (at least 8 out of 9)\")\n",
        "    content.append(\"\")\n",
        "    content.append(\"3. MEAN ABSOLUTE ERROR (MAE): Average distance from benchmark\")\n",
        "    content.append(\"   Target: < 0.5 points per dimension\")\n",
        "    content.append(\"   \")\n",
        "    content.append(\"   Formula: Sum of |your score - benchmark| Ã· number of summaries\")\n",
        "    content.append(\"\")\n",
        "    content.append(\"4. PATTERNS IN DISCREPANCIES:\")\n",
        "    content.append(\"   - Do you tend to score higher or lower than benchmarks?\")\n",
        "    content.append(\"   - Are discrepancies concentrated in specific dimensions?\")\n",
        "    content.append(\"   - Are errors larger for certain score levels?\")\n",
        "    content.append(\"\")\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"\")\n",
        "\n",
        "    # Next steps\n",
        "    content.append(\"NEXT STEPS AFTER COMPARISON\")\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"\")\n",
        "    content.append(\"1. Calculate your agreement metrics\")\n",
        "    content.append(\"2. Identify patterns in discrepancies\")\n",
        "    content.append(\"3. Create decision rules for borderline cases\")\n",
        "    content.append(\"4. Review rubric areas where you struggled\")\n",
        "    content.append(\"5. Proceed to Practice Round 2 with refined approach\")\n",
        "    content.append(\"6. After Round 2, assess readiness for full validation scoring\")\n",
        "    content.append(\"\")\n",
        "    content.append(\"=\" * 80)\n",
        "\n",
        "    # Write to file\n",
        "    with open(output_txt, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(content))\n",
        "\n",
        "    print(f\"âœ“ Benchmark scores file created: {output_txt}\")\n",
        "    print(f\"  Total summaries: {len(df)}\")\n",
        "    print(f\"  Exemplars: {len(exemplars)}\")\n",
        "    print(f\"  Practice Round 1: {len(round_1)}\")\n",
        "    print(f\"  Practice Round 2: {len(round_2)}\")\n",
        "    print(f\"\\nScore distribution:\")\n",
        "    for score, count in score_counts.items():\n",
        "        print(f\"  Score {score}: {count}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create the benchmark scores file\n",
        "    create_benchmark_scores_file(CALIBRATION_SUBSET_PATH, OUTPUT_PATH)\n",
        "\n",
        "    print(\"\\nâœ“ Generation complete!\")\n",
        "    print(f\"\\nReminder: DO NOT open {OUTPUT_PATH} until after blind scoring!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCC-EuslhC0D",
        "outputId": "a8251aa8-abb6-4851-8b2b-98645f9cab05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Benchmark scores file created: /content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_2/Phase2_Calibration/CALIBRATION_BENCHMARK_SCORES.txt\n",
            "  Total summaries: 12\n",
            "  Exemplars: 4\n",
            "  Practice Round 1: 4\n",
            "  Practice Round 2: 4\n",
            "\n",
            "Score distribution:\n",
            "  Score 1: 2\n",
            "  Score 2: 3\n",
            "  Score 3: 3\n",
            "  Score 4: 2\n",
            "  Score 5: 1\n",
            "  Score 6: 1\n",
            "\n",
            "âœ“ Generation complete!\n",
            "\n",
            "Reminder: DO NOT open /content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_2/Phase2_Calibration/CALIBRATION_BENCHMARK_SCORES.txt until after blind scoring!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "25-Summary Validation Subset Selector\n",
        "Stratified sampling from 60-summary validation set for accelerated timeline\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION - Adjust these paths for your Google Drive setup\n",
        "# ============================================================================\n",
        "\n",
        "# Path to your validation_set_combined_60.csv in Google Drive\n",
        "VALIDATION_DATASET_PATH = \"/content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_1/data/dataset/validation_set_combined_60.csv\"\n",
        "\n",
        "# Output directory in Google Drive\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_2/data\"\n",
        "\n",
        "# Random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Target distribution for 25 summaries (proportional to original 60)\n",
        "TARGET_DISTRIBUTION = {\n",
        "    1: 3,   # From 8 available\n",
        "    2: 8,   # From 18 available\n",
        "    3: 8,   # From 20 available\n",
        "    4: 4,   # From 10 available\n",
        "    5: 1,   # From 3 available\n",
        "    6: 1    # From 1 available\n",
        "}\n",
        "# Total = 25 summaries\n",
        "\n",
        "# ============================================================================\n",
        "\n",
        "def select_validation_subset(input_csv, target_dist, random_seed=42):\n",
        "    \"\"\"\n",
        "    Select stratified 25-summary subset from 60-summary validation set.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    input_csv : str\n",
        "        Path to validation_set_combined_60.csv\n",
        "    target_dist : dict\n",
        "        Target number of summaries per score level\n",
        "    random_seed : int\n",
        "        Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        Selected subset of 25 summaries\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"FAST-TRACK VALIDATION SUBSET SELECTION\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "\n",
        "    # Read full validation dataset\n",
        "    print(f\"Reading validation dataset...\")\n",
        "    print(f\"  Path: {input_csv}\")\n",
        "    df = pd.read_csv(input_csv)\n",
        "    print(f\"âœ“ Loaded {len(df)} summaries\")\n",
        "    print()\n",
        "\n",
        "    # Set random seed\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    # Display current distribution\n",
        "    print(\"Current score distribution (60 summaries):\")\n",
        "    score_dist = df['score'].value_counts().sort_index()\n",
        "    for score, count in score_dist.items():\n",
        "        auth_count = len(df[(df['score'] == score) & (df['synthetic_flag'] == 0)])\n",
        "        synth_count = len(df[(df['score'] == score) & (df['synthetic_flag'] == 1)])\n",
        "        print(f\"  Score {score}: {count} total ({auth_count} authentic, {synth_count} synthetic)\")\n",
        "    print()\n",
        "\n",
        "    # Stratified sampling by score\n",
        "    print(\"Target distribution (25 summaries):\")\n",
        "    for score, target in target_dist.items():\n",
        "        print(f\"  Score {score}: {target} summaries\")\n",
        "    print()\n",
        "\n",
        "    print(\"Selecting summaries...\")\n",
        "    selected_dfs = []\n",
        "\n",
        "    for score, target_count in target_dist.items():\n",
        "        # Get all summaries with this score\n",
        "        score_df = df[df['score'] == score].copy()\n",
        "\n",
        "        if len(score_df) < target_count:\n",
        "            print(f\"  âš  Warning: Only {len(score_df)} summaries available for score {score} (need {target_count})\")\n",
        "            selected = score_df\n",
        "        else:\n",
        "            # Randomly sample target_count summaries\n",
        "            selected = score_df.sample(n=target_count, random_state=random_seed)\n",
        "\n",
        "        selected_dfs.append(selected)\n",
        "\n",
        "        auth_selected = len(selected[selected['synthetic_flag'] == 0])\n",
        "        synth_selected = len(selected[selected['synthetic_flag'] == 1])\n",
        "        print(f\"  âœ“ Score {score}: Selected {len(selected)} ({auth_selected} authentic, {synth_selected} synthetic)\")\n",
        "\n",
        "    # Combine all selected summaries\n",
        "    subset_df = pd.concat(selected_dfs, ignore_index=True)\n",
        "\n",
        "    # Shuffle the final subset\n",
        "    subset_df = subset_df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
        "\n",
        "    # Add validation_id for tracking\n",
        "    subset_df['validation_id'] = [f'VAL_{str(i+1).zfill(2)}' for i in range(len(subset_df))]\n",
        "\n",
        "    print()\n",
        "    print(\"=\" * 80)\n",
        "    print(\"SELECTION COMPLETE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Total selected: {len(subset_df)} summaries\")\n",
        "    print()\n",
        "\n",
        "    # Final distribution summary\n",
        "    print(\"Final subset distribution:\")\n",
        "    for score in sorted(subset_df['score'].unique()):\n",
        "        count = len(subset_df[subset_df['score'] == score])\n",
        "        auth_count = len(subset_df[(subset_df['score'] == score) & (subset_df['synthetic_flag'] == 0)])\n",
        "        synth_count = len(subset_df[(subset_df['score'] == score) & (subset_df['synthetic_flag'] == 1)])\n",
        "        print(f\"  Score {score}: {count} ({auth_count} authentic, {synth_count} synthetic)\")\n",
        "\n",
        "    total_auth = len(subset_df[subset_df['synthetic_flag'] == 0])\n",
        "    total_synth = len(subset_df[subset_df['synthetic_flag'] == 1])\n",
        "    print()\n",
        "    print(f\"Overall: {total_auth} authentic ({total_auth/len(subset_df)*100:.1f}%), \"\n",
        "          f\"{total_synth} synthetic ({total_synth/len(subset_df)*100:.1f}%)\")\n",
        "\n",
        "    return subset_df\n",
        "\n",
        "\n",
        "def create_scoring_text_file(subset_df, output_txt):\n",
        "    \"\"\"\n",
        "    Create formatted text file for manual scoring.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    subset_df : pd.DataFrame\n",
        "        Selected validation subset\n",
        "    output_txt : str\n",
        "        Path for output text file\n",
        "    \"\"\"\n",
        "\n",
        "    content = []\n",
        "\n",
        "    # Header\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"FAST-TRACK VALIDATION SET - 25 SUMMARIES FOR SCORING\")\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"\")\n",
        "    content.append(\"Instructions:\")\n",
        "    content.append(\"1. Score each summary across all 4 dimensions using your calibrated approach\")\n",
        "    content.append(\"2. Record scores in your scoring template spreadsheet\")\n",
        "    content.append(\"3. Document brief rationale for borderline cases\")\n",
        "    content.append(\"4. These scores will be your ground truth for LLM validation\")\n",
        "    content.append(\"\")\n",
        "    content.append(\"Timeline: Complete all 25 by end of day Monday, December 2\")\n",
        "    content.append(\"Estimated time: 6-8 hours (15-20 min per summary)\")\n",
        "    content.append(\"\")\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"\")\n",
        "\n",
        "    # Each summary\n",
        "    for idx, row in subset_df.iterrows():\n",
        "        content.append(\"\")\n",
        "        content.append(\"=\" * 80)\n",
        "        content.append(f\"{row['validation_id']}: {row['essay_id']}\")\n",
        "        content.append(\"=\" * 80)\n",
        "        content.append(f\"Original Score: {row['score']}\")\n",
        "        content.append(f\"Source: {'Authentic' if row['synthetic_flag'] == 0 else 'Synthetic'}\")\n",
        "        content.append(f\"Word Count: {row['word_count']}\")\n",
        "        if row['synthetic_flag'] == 1 and pd.notna(row.get('target_error_pattern')):\n",
        "            content.append(f\"Error Pattern: {row['target_error_pattern']}\")\n",
        "        content.append(\"\")\n",
        "        content.append(\"-\" * 80)\n",
        "        content.append(\"SUMMARY TEXT:\")\n",
        "        content.append(\"-\" * 80)\n",
        "        content.append(\"\")\n",
        "        content.append(row['full_text'])\n",
        "        content.append(\"\")\n",
        "        content.append(\"-\" * 80)\n",
        "        content.append(\"YOUR SCORES (Complete after reading):\")\n",
        "        content.append(\"-\" * 80)\n",
        "        content.append(\"Completeness (1-5): _____\")\n",
        "        content.append(\"Accuracy (1-5): _____\")\n",
        "        content.append(\"Coherence (1-5): _____\")\n",
        "        content.append(\"Conciseness (1-5): _____\")\n",
        "        content.append(\"\")\n",
        "        content.append(\"Brief rationale/notes:\")\n",
        "        content.append(\"\")\n",
        "        content.append(\"\")\n",
        "        content.append(\"=\" * 80)\n",
        "        content.append(\"\")\n",
        "\n",
        "    # Footer\n",
        "    content.append(\"\")\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"END OF VALIDATION SET\")\n",
        "    content.append(\"=\" * 80)\n",
        "    content.append(\"\")\n",
        "    content.append(\"Next steps after scoring:\")\n",
        "    content.append(\"1. Transfer scores to spreadsheet\")\n",
        "    content.append(\"2. Begin LLM prompt design (Tuesday)\")\n",
        "    content.append(\"3. Test initial prompt on 5 summaries (Tuesday)\")\n",
        "    content.append(\"4. Prepare progress update presentation (Wednesday)\")\n",
        "\n",
        "    # Write to file\n",
        "    with open(output_txt, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(content))\n",
        "\n",
        "    print(f\"âœ“ Scoring text file created\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "    print()\n",
        "\n",
        "    # Define output paths\n",
        "    output_csv = os.path.join(OUTPUT_DIR, \"validation_subset_25.csv\")\n",
        "    output_txt = os.path.join(OUTPUT_DIR, \"validation_subset_25_for_scoring.txt\")\n",
        "\n",
        "    # Select subset\n",
        "    subset_df = select_validation_subset(\n",
        "        VALIDATION_DATASET_PATH,\n",
        "        TARGET_DISTRIBUTION,\n",
        "        RANDOM_SEED\n",
        "    )\n",
        "\n",
        "    # Save CSV\n",
        "    print(f\"\\nSaving subset CSV...\")\n",
        "    subset_df.to_csv(output_csv, index=False)\n",
        "    print(f\"âœ“ CSV saved: {output_csv}\")\n",
        "    print(f\"  {len(subset_df)} summaries\")\n",
        "\n",
        "    # Create scoring text file\n",
        "    print(f\"\\nCreating scoring text file...\")\n",
        "    create_scoring_text_file(subset_df, output_txt)\n",
        "    print(f\"âœ“ Text file saved: {output_txt}\")\n",
        "\n",
        "    # Summary statistics\n",
        "    print()\n",
        "    print(\"=\" * 80)\n",
        "    print(\"FILES CREATED IN GOOGLE DRIVE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"1. validation_subset_25.csv - Subset data for analysis\")\n",
        "    print(f\"2. validation_subset_25_for_scoring.txt - Formatted for manual scoring\")\n",
        "    print()\n",
        "    print(f\"Location: {OUTPUT_DIR}\")\n",
        "    print()\n",
        "    print(\"=\" * 80)\n",
        "    print(\"NEXT STEPS - FAST-TRACK SCHEDULE\")\n",
        "    print(\"=\" * 80)\n",
        "    print()\n",
        "    print(\"ðŸ“… MONDAY DEC 1 (Tomorrow):\")\n",
        "    print(\"   â€¢ Score all 25 summaries (6-8 hours)\")\n",
        "    print(\"   â€¢ Use your calibrated approach from practice rounds\")\n",
        "    print(\"   â€¢ Document scores in spreadsheet as you go\")\n",
        "    print()\n",
        "    print(\"ðŸ“… TUESDAY DEC 2:\")\n",
        "    print(\"   â€¢ Design base LLM evaluation prompt\")\n",
        "    print(\"   â€¢ Set up Llama 3.1 8B in Colab\")\n",
        "    print(\"   â€¢ Test on 5 summaries\")\n",
        "    print(\"   â€¢ Calculate initial agreement metrics\")\n",
        "    print()\n",
        "    print(\"ðŸ“… WEDNESDAY DEC 3:\")\n",
        "    print(\"   â€¢ Prepare progress update presentation\")\n",
        "    print(\"   â€¢ Iterate on prompt based on results\")\n",
        "    print(\"   â€¢ DELIVERABLE: Progress update\")\n",
        "    print()\n",
        "    print(\"ðŸŽ¯ You're on track for the December 10 demo!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob1Rt8laGGgl",
        "outputId": "30f2b668-8bc2-4117-aec8-fd625ff24080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Output directory: /content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_2/data\n",
            "\n",
            "================================================================================\n",
            "FAST-TRACK VALIDATION SUBSET SELECTION\n",
            "================================================================================\n",
            "\n",
            "Reading validation dataset...\n",
            "  Path: /content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_1/data/dataset/validation_set_combined_60.csv\n",
            "âœ“ Loaded 60 summaries\n",
            "\n",
            "Current score distribution (60 summaries):\n",
            "  Score 1: 8 total (5 authentic, 3 synthetic)\n",
            "  Score 2: 18 total (11 authentic, 7 synthetic)\n",
            "  Score 3: 20 total (12 authentic, 8 synthetic)\n",
            "  Score 4: 10 total (6 authentic, 4 synthetic)\n",
            "  Score 5: 3 total (2 authentic, 1 synthetic)\n",
            "  Score 6: 1 total (1 authentic, 0 synthetic)\n",
            "\n",
            "Target distribution (25 summaries):\n",
            "  Score 1: 3 summaries\n",
            "  Score 2: 8 summaries\n",
            "  Score 3: 8 summaries\n",
            "  Score 4: 4 summaries\n",
            "  Score 5: 1 summaries\n",
            "  Score 6: 1 summaries\n",
            "\n",
            "Selecting summaries...\n",
            "  âœ“ Score 1: Selected 3 (3 authentic, 0 synthetic)\n",
            "  âœ“ Score 2: Selected 8 (4 authentic, 4 synthetic)\n",
            "  âœ“ Score 3: Selected 8 (3 authentic, 5 synthetic)\n",
            "  âœ“ Score 4: Selected 4 (3 authentic, 1 synthetic)\n",
            "  âœ“ Score 5: Selected 1 (1 authentic, 0 synthetic)\n",
            "  âœ“ Score 6: Selected 1 (1 authentic, 0 synthetic)\n",
            "\n",
            "================================================================================\n",
            "SELECTION COMPLETE\n",
            "================================================================================\n",
            "Total selected: 25 summaries\n",
            "\n",
            "Final subset distribution:\n",
            "  Score 1: 3 (3 authentic, 0 synthetic)\n",
            "  Score 2: 8 (4 authentic, 4 synthetic)\n",
            "  Score 3: 8 (3 authentic, 5 synthetic)\n",
            "  Score 4: 4 (3 authentic, 1 synthetic)\n",
            "  Score 5: 1 (1 authentic, 0 synthetic)\n",
            "  Score 6: 1 (1 authentic, 0 synthetic)\n",
            "\n",
            "Overall: 15 authentic (60.0%), 10 synthetic (40.0%)\n",
            "\n",
            "Saving subset CSV...\n",
            "âœ“ CSV saved: /content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_2/data/validation_subset_25.csv\n",
            "  25 summaries\n",
            "\n",
            "Creating scoring text file...\n",
            "âœ“ Scoring text file created\n",
            "âœ“ Text file saved: /content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_2/data/validation_subset_25_for_scoring.txt\n",
            "\n",
            "================================================================================\n",
            "FILES CREATED IN GOOGLE DRIVE\n",
            "================================================================================\n",
            "1. validation_subset_25.csv - Subset data for analysis\n",
            "2. validation_subset_25_for_scoring.txt - Formatted for manual scoring\n",
            "\n",
            "Location: /content/drive/MyDrive/Courses/2025/3_Fall/EDUC_6192_Large_Language_Model_Applications_in_Education/Project/Phase_2/data\n",
            "\n",
            "================================================================================\n",
            "NEXT STEPS - FAST-TRACK SCHEDULE\n",
            "================================================================================\n",
            "\n",
            "ðŸ“… MONDAY DEC 1 (Tomorrow):\n",
            "   â€¢ Score all 25 summaries (6-8 hours)\n",
            "   â€¢ Use your calibrated approach from practice rounds\n",
            "   â€¢ Document scores in spreadsheet as you go\n",
            "\n",
            "ðŸ“… TUESDAY DEC 2:\n",
            "   â€¢ Design base LLM evaluation prompt\n",
            "   â€¢ Set up Llama 3.1 8B in Colab\n",
            "   â€¢ Test on 5 summaries\n",
            "   â€¢ Calculate initial agreement metrics\n",
            "\n",
            "ðŸ“… WEDNESDAY DEC 3:\n",
            "   â€¢ Prepare progress update presentation\n",
            "   â€¢ Iterate on prompt based on results\n",
            "   â€¢ DELIVERABLE: Progress update\n",
            "\n",
            "ðŸŽ¯ You're on track for the December 10 demo!\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}