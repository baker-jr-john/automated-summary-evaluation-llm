# ðŸ“š References

Andrade, H. G. (2001). *The effects of instructional rubrics on learning to write.* Current Issues in Education, 4. https://cie.asu.edu/ojs/index.php/cieatasu/article/view/1630

Andrade, H. G., & Boulay, B. A. (2003). Role of rubric-referenced self-assessment in learning to write. *The Journal of Educational Research, 97*(1), 21â€“30. https://doi.org/10.1080/00220670309596625

Bangert-Drowns, R. L., Hurley, M. M., & Wilkinson, B. (2004). The effects of school-based writing-to-learn interventions on academic achievement: A meta-analysis. *Review of Educational Research, 74*(1), 29â€“58. https://doi.org/10.3102/00346543074001029

Bell, C., James, J., Taylor, E. S., & Wyckoff, J. (2023). *Measuring returns to experience using supervisor ratings of observed performance: The case of classroom teachers* (EdWorkingPaper No. 23-715). Annenberg Institute for School Reform at Brown University. https://doi.org/10.26300/9abf-tk90

Benedetto, L., Aradelli, G., Donvito, A., Lucchetti, A., Cappelli, A., & Buttery, P. (2024, November). Using LLMs to simulate studentsâ€™ responses to exam questions. In *Findings of the Association for Computational Linguistics: EMNLP 2024* (pp. 11351â€“11368). https://doi.org/10.18653/v1/2024.findings-emnlp.663

Blatchford, P., & Catchpole, G. (2003). Class size and classroom processes. In *International Handbook of Educational Research in the Asia-Pacific Region* (pp. 741â€“754). Dordrecht: Springer Netherlands. https://doi.org/10.1007/978-94-017-3368-7_51

Chamieh, I., Zesch, T., & Giebermann, K. (2024, June). LLMs in short answer scoring: Limitations and promise of zero-shot and few-shot approaches. In *Proceedings of the 19th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2024)* (pp. 309â€“315).

Cohen, C., Hutchins, N., Le, T., & Biswas, G. (2024). A chain-of-thought prompting approach with LLMs for evaluating studentsâ€™ formative assessment responses in science. *arXiv preprint arXiv:2403.14565.*

Cohen, E. G., Lotan, R. A., Abram, P. L., Scarloss, B. A., & Schultz, S. E. (2002). Can groups learn? *Teachers College Record, 104*(6), 1045â€“1068. https://doi.org/10.1111/1467-9620.00196

EdWeek Research Center. (2022). *1st annual Merrimack College Teacher Survey: 2022 results.* https://fs24.formsite.com/edweek/images/WP-Merrimack_College-Todays_Teachers_Are_Deeply_Disillusioned_Survey_Data_Confirms.pdf

Garcia-Alcoser, M. E., GhojoghNejad, M., Tushar, F. I., Kim, D., Lafata, K. J., Rubin, G. D., & Lo, J. Y. (2025). Evaluating large language models for zero-shot disease labeling in CT radiology reports across organ systems. *arXiv preprint arXiv:2506.03259.*

Graham, S., Bruch, J., Fitzgerald, J., Friedrich, L., Furgeson, J., Greene, K., Kim, J., Lyskawa, J., Olson, C. B., & Smither Wulsin, C. (2016). *Teaching secondary students to write effectively (NCEE 2017-4002).* Institute of Education Sciences, U.S. Department of Education.

Graham, S., & Hebert, M. (2010). *Writing to read: Evidence for how writing can improve reading.* Carnegie Corporation of New York.

Hackl, V., MÃ¼ller, A. E., Granitzer, M., & Sailer, M. (2023). Is GPT-4 a reliable rater? Evaluating consistency in GPT-4â€™s text ratings. *Frontiers in Education, 8*, 1272229. https://doi.org/10.3389/feduc.2023.1272229

Han, J., Yoo, H., Myung, J., Kim, M., Lim, H., Kim, Y., ... & Oh, A. (2023). LLM-as-a-tutor in EFL writing education: Focusing on evaluation of studentâ€“LLM interaction. *arXiv preprint arXiv:2310.05191.*

Hashemi, H., Eisner, J., Rosset, C., Van Durme, B., & Kedzie, C. (2024). *LLM-Rubric: A multidimensional, calibrated approach to automated evaluation of natural language texts.* *arXiv preprint arXiv:2501.00274.*

Jiang, L., & Bosch, N. (2024, July). Short answer scoring with GPT-4. In *Proceedings of the Eleventh ACM Conference on Learning@Scale* (pp. 438â€“442). https://doi.org/10.1145/3657604.3665565

Jiang, J., & Sporte, S. (2016). *Teacher evaluation in Chicago: Differences in observation and value-added scores by teacher, student, and school characteristics.* University of Chicago Consortium on School Research.

Liu, N., Sonkar, S., & Baraniuk, R. (2025, July). Do LLMs make mistakes like students? Exploring natural alignments between language models and human error patterns. In *International Conference on Artificial Intelligence in Education* (pp. 364â€“377). Springer Nature.

Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., & Zhu, C. (2023). G-Eval: NLG evaluation using GPT-4 with better human alignment. *arXiv preprint arXiv:2303.16634.*

Lynam, S., & Cachia, M. (2018). Studentsâ€™ perceptions of the role of assessments in higher education. *Assessment & Evaluation in Higher Education, 43*(2), 223â€“234. https://doi.org/10.1080/02602938.2017.1339772

Meyer, J., Jansen, T., Schiller, R., Liebenow, L. W., Steinbach, M., Horbach, A., & Fleckenstein, J. (2024). Using LLMs to bring evidence-based feedback into the classroom: AI-generated feedback increases secondary studentsâ€™ text revision, motivation, and positive emotions. *Computers & Education: Artificial Intelligence, 6*, 100199. https://doi.org/10.1016/j.caeai.2024.100199

Parikh, N., Fernandez, N., Scarlatos, A., Woodhead, S., & Lan, A. (2025). LookAlike: Consistent distractor generation in math MCQs. *arXiv preprint arXiv:2505.01903.*

Quitadamo, I. J., & Kurtz, M. J. (2007). Learning to improve: Using writing to increase critical thinking performance in general education biology. *CBEâ€”Life Sciences Education, 6*(2), 140â€“154. https://doi.org/10.1187/cbe.06-11-0203

Reynders, G., Lantz, J., Ruder, S. M., Stanford, C. L., & Cole, R. S. (2020). Rubrics to assess critical thinking and information processing in undergraduate STEM courses. *International Journal of STEM Education, 7*(1), 9. https://doi.org/10.1186/s40594-020-00207-3

Sadler, P. M., & Good, E. (2006). The impact of self- and peer-grading on student learning. *Educational Assessment, 11*(1), 1â€“31. https://doi.org/10.1207/s15326977ea1101_1

Shibata, T., & Miyamura, Y. (2025). LCES: Zero-shot automated essay scoring via pairwise comparisons using large language models. *arXiv preprint arXiv:2505.08498.*

U.S. Department of Education. (2023). *Artificial intelligence and the future of teaching and learning.* Office of Educational Technology. https://www.ed.gov/media/document/ai-reportpdf-43861.pdf

U.S. Department of Education, National Center for Education Statistics. (n.d.). *Average class size in public Kâ€“12 schools, by school level, class type, and state: 2020â€“21.* https://nces.ed.gov/surveys/ntps/estable/table/ntps/ntps2021_sflt07_t1s

van Helvoort, J. (2010). A scoring rubric for performance assessment of information literacy in Dutch higher education. *Journal of Information Literacy, 4*(1). https://doi.org/10.11645/4.1.1256

Voelkel, S., Varga-Atkins, T., & Mello, L. V. (2020). Students tell us what good written feedback looks like. *FEBS Open Bio, 10*(5), 692â€“706. https://doi.org/10.1002/2211-5463.12840

Weaver, M. R. (2006). Do students value feedback? Student perceptions of tutorsâ€™ written responses. *Assessment & Evaluation in Higher Education, 31*(3), 379â€“394. https://doi.org/10.1080/02602930500353061

Wei, Y., Pearl, D., Beckman, M., & Passonneau, R. J. (2025). Concept-based rubrics improve LLM formative assessment and data synthesis. *arXiv preprint arXiv:2504.03877.*

Williams, A. (2024). Delivering effective student feedback in higher education: An evaluation of the challenges and best practice. *International Journal of Research in Education and Science, 10*(2), 473â€“501.

Yan, Z., Lao, H., Panadero, E., FernÃ¡ndez-Castilla, B., Yang, L., & Yang, M. (2022). Effects of self-assessment and peer-assessment interventions on academic performance: A meta-analysis. *Educational Research Review, 37*, 100484. https://doi.org/10.1016/j.edurev.2022.100484

Yancey, K. P., Laflair, G., Verardi, A., & Burstein, J. (2023, July). Rating short L2 essays on the CEFR scale with GPT-4. In *Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)* (pp. 576â€“584). Association for Computational Linguistics.
